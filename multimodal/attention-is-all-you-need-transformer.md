# attention is all you need(transformer)

仅仅用了注意力机制而不是循环或者卷积

第一个只依赖于自注意力

point : multi-headed self-attention层





时序模型现在用的最多的是rnn(一个词一个词的分析)

transofrmer类似于多头的注意力，约等于多输出通道





使用编码器解码器架构



1.inputs

{% embed url="https://arxiv.org/pdf/1706.03762.pdf" %}
