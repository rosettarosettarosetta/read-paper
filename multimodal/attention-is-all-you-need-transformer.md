# attention is all you need(transformer)

仅仅用了注意力机制而不是循环或者卷积

point : multi-headed self-attention层





时序模型现在用的最多的是cnn



{% embed url="https://arxiv.org/pdf/1706.03762.pdf" %}
